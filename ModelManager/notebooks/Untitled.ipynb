{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.133.21.234:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0-preview2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>LogAnalyst</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f397ccc8810>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkSession = SparkSession.builder\\\n",
    "                            .appName(\"LogAnalyst\")\\\n",
    "                            .master(\"local\")\\\n",
    "                            .getOrCreate()\n",
    "sparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sparkSession.read.option(\"header\", \"false\").csv(\"hdfs://localhost:9000/data/task-event\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----+-------+---+------------+---+--------------------+---+---+-----+-------+--------------------+----+\n",
      "|_c0|_c1| _c2|    _c3|_c4|         _c5|_c6|                 _c7|_c8|_c9| _c10|   _c11|                _c12|_c13|\n",
      "+---+---+----+-------+---+------------+---+--------------------+---+---+-----+-------+--------------------+----+\n",
      "|  0|  0| 2.0|3418309|  0|4155527081.0|  0|70s3v5qRyCO/1PCdI...|  3|  9| null|   null|                null|null|\n",
      "|  1|  0| 2.0|3418309|  1| 329150663.0|  0|70s3v5qRyCO/1PCdI...|  3|  9| null|   null|                null|null|\n",
      "|  2|  0|null|3418314|  0|3938719206.0|  0|70s3v5qRyCO/1PCdI...|  3|  9|0.125|0.07446|0.000424399999999...| 0.0|\n",
      "|  3|  0|null|3418314|  1| 351618647.0|  0|70s3v5qRyCO/1PCdI...|  3|  9|0.125|0.07446|0.000424399999999...| 0.0|\n",
      "|  4|  0| 2.0|3418319|  0| 431052910.0|  0|70s3v5qRyCO/1PCdI...|  3|  9| null|   null|                null|null|\n",
      "|  5|  0| 2.0|3418319|  1| 257348783.0|  0|70s3v5qRyCO/1PCdI...|  3|  9| null|   null|                null|null|\n",
      "|  6|  0| 2.0|3418324|  0|5655258253.0|  0|70s3v5qRyCO/1PCdI...|  3|  9| null|   null|                null|null|\n",
      "|  7|  0| 2.0|3418324|  1|3550322224.0|  0|70s3v5qRyCO/1PCdI...|  3|  9| null|   null|                null|null|\n",
      "|  8|  0| 2.0|3418329|  0|   1303745.0|  0|70s3v5qRyCO/1PCdI...|  3|  9| null|   null|                null|null|\n",
      "|  9|  0| 2.0|3418329|  1|3894543095.0|  0|70s3v5qRyCO/1PCdI...|  3|  9| null|   null|                null|null|\n",
      "+---+---+----+-------+---+------------+---+--------------------+---+---+-----+-------+--------------------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------------------+\n",
      "|   _c10|   _c11|                _c12|\n",
      "+-------+-------+--------------------+\n",
      "|  0.125|0.07446|0.000424399999999...|\n",
      "|  0.125|0.07446|0.000424399999999...|\n",
      "|0.03125|0.08691|0.000454899999999...|\n",
      "|0.03125|0.08691|0.000454899999999...|\n",
      "|0.03125|0.08691|0.000454899999999...|\n",
      "+-------+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_selected = df.select('_c10', '_c11', '_c12')\n",
    "df_dropna = df_selected.dropna()\n",
    "df_dropna.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------------------+\n",
      "|   _c10|   _c11|                _c12|\n",
      "+-------+-------+--------------------+\n",
      "|  0.125|0.07446|4.243999999999999...|\n",
      "|  0.125|0.07446|4.243999999999999...|\n",
      "|0.03125|0.08691|4.548999999999999...|\n",
      "+-------+-------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "for c in df_dropna.columns: \n",
    "    df_dropna = df_dropna.withColumn(c, col(c).cast(\"double\"))\n",
    "    \n",
    "# df_dropna.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler, VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "stages = [] \n",
    "num_cols = df_dropna.columns\n",
    "# for col in num_cols: \n",
    "#     scaler = StandardScaler(inputCol=col, outputCol=col+'_transformed', withStd=True, withMean=False)\n",
    "#     stages.append(scaler)\n",
    "    \n",
    "feature_cols = [col for col in num_cols]\n",
    "vector_assembler = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
    "stages.append(vector_assembler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+--------+--------------------+\n",
      "|  _c10|   _c11|    _c12|            features|\n",
      "+------+-------+--------+--------------------+\n",
      "|0.1125|0.03802|3.033E-4|[0.1125,0.03802,3...|\n",
      "|0.1125|0.03802|3.033E-4|[0.1125,0.03802,3...|\n",
      "|0.1125|0.03802|3.033E-4|[0.1125,0.03802,3...|\n",
      "|0.1125|0.03802|3.033E-4|[0.1125,0.03802,3...|\n",
      "|0.1125|0.03802|3.033E-4|[0.1125,0.03802,3...|\n",
      "+------+-------+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=stages) \n",
    "transformer = pipeline.fit(df_dropna)\n",
    "new_df = transformer.transform(df_dropna)\n",
    "new_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussians shown as a DataFrame: \n",
      "-RECORD 0--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " mean | [0.076264,0.037794054054054074,1.507426486486487E-4]                                                                                                                                                           \n",
      " cov  | 0.001016320106810814  2.924396334054038E-4   3.844277061837835E-6   \n",
      "2.924396334054038E-4  7.925422894024818E-4   1.5342560171541173E-6  \n",
      "3.844277061837835E-6  1.5342560171541173E-6  1.6731754307633302E-8   \n",
      "-RECORD 1--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " mean | [0.03125,0.049620000000000226,3.1660000000000016E-4]                                                                                                                                                           \n",
      " cov  | 0.0  0.0                      0.0                      \n",
      "0.0  -2.900582676948607E-17   -1.1330401081830497E-19  \n",
      "0.0  -1.1330401081830497E-19  -2.899752432041749E-22                                           \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import GaussianMixture\n",
    "\n",
    "gmm = GaussianMixture().setK(2).setSeed(538009335)\n",
    "model = gmm.fit(new_df)\n",
    "\n",
    "print(\"Gaussians shown as a DataFrame: \")\n",
    "model.gaussiansDF.show(truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = model.transform(new_df).select(\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|        prediction|\n",
      "+-------+------------------+\n",
      "|  count|               259|\n",
      "|   mean|0.8571428571428571|\n",
      "| stddev|0.3506046035634262|\n",
      "|    min|                 0|\n",
      "|    max|                 1|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "labels.describe().show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
